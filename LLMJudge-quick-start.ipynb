{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgTuRyUk9Btx"
      },
      "outputs": [],
      "source": [
        "!pip install jsonlines\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6vGL7iI7oyU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import jsonlines\n",
        "from tqdm import tqdm\n",
        "\n",
        "import transformers\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzr9gCTk-n8D"
      },
      "outputs": [],
      "source": [
        "torch.set_default_device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1qm0TyC7xuQ"
      },
      "source": [
        "## Load Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQt0I7z170cq"
      },
      "outputs": [],
      "source": [
        "docid_to_doc = dict()\n",
        "\n",
        "with jsonlines.open('./data/llm4eval_document_2024.jsonl', 'r') as document_file:\n",
        "  for obj in document_file:\n",
        "    docid_to_doc[obj['docid']] = obj['doc']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL8wuWsk7W3A"
      },
      "source": [
        "## Load Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJxWHC0v7frD"
      },
      "outputs": [],
      "source": [
        "query_data = pd.read_csv(\"./data/llm4eval_query_2024.txt\", sep=\"\\t\", header=None, names=['qid', 'qtext'])\n",
        "qid_to_query = dict(zip(query_data.qid, query_data.qtext))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLugN3Ve-io9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdEWa_ksllgY"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"You are a search quality rater evaluating the relevance of passages. Given a query and passage, you must provide a score on an integer scale of 0 to 3 with the following meanings:\n",
        "\n",
        "    3 = Perfectly relevant: The passage is dedicated to the query and contains the exact answer.\n",
        "    2 = Highly relevant: The passage has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information.\n",
        "    1 = Related: The passage seems related to the query but does not answer it.\n",
        "    0 = Irrelevant: The passage has nothing to do with the query\n",
        "\n",
        "    Assume that you are writing an answer to the query. If the passage seems to be related to the query but does not include any answer to the query, mark it 1. If you would use any of the information contained in the passage in such an asnwer, mark it 2. If the passage is primarily about the query, or contains vital information about the topic, mark it 3. Otherwise, mark it 0.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtBlobzh-jH5"
      },
      "outputs": [],
      "source": [
        "def get_prompt(query, passage):\n",
        "    return f\"\"\"Please rate how the given passage is relevant to the query. The output must be only a score that indicate how relevant they are.\n",
        "\n",
        "    Query: {query}\n",
        "    Passage: {passage}\n",
        "\n",
        "    Score:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6idCEfr9mWVs"
      },
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fbmKO-Ho6Q6"
      },
      "outputs": [],
      "source": [
        "def get_relevance_score(prompt):\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": system_message},\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "\n",
        "  prompt = pipeline.tokenizer.apply_chat_template(\n",
        "          messages,\n",
        "          tokenize=False,\n",
        "          add_generation_prompt=True\n",
        "  )\n",
        "\n",
        "  terminators = [\n",
        "      pipeline.tokenizer.eos_token_id,\n",
        "      pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  outputs = pipeline(\n",
        "      prompt,\n",
        "      max_new_tokens=256,\n",
        "      eos_token_id=terminators,\n",
        "      pad_token_id=128009,\n",
        "      do_sample=True,\n",
        "      temperature=0.6,\n",
        "      top_p=0.9,\n",
        "  )\n",
        "\n",
        "  return outputs[0][\"generated_text\"][len(prompt):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6Z3P4CLoRFP"
      },
      "outputs": [],
      "source": [
        "test_qrel = pd.read_csv(\"./data/llm4eval_test_qrel_2024.txt\", sep=\" \", header=None, names=['qid', 'Q0', 'docid'])\n",
        "test_qrel.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWibmM9hoi7f"
      },
      "outputs": [],
      "source": [
        "with open('llm4eval_test_qrel_results.txt', 'w') as result_file:\n",
        "  for eachline in tqdm(test_qrel.itertuples(index=True)):\n",
        "    qidx = eachline.qid\n",
        "    docidx = eachline.docid\n",
        "    prompt = get_prompt(query=qid_to_query[qidx], passage=docid_to_doc[docidx])\n",
        "    pred_score = get_relevance_score(prompt)\n",
        "    result_file.write(f\"{qidx} 0 {docidx} {pred_score}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
